{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata \n",
    "mnist=fetch_mldata('MNIST original') \n",
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y= mnist[\"data\"],mnist[\"target\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output dataset\n",
    "X.shape (70000,784) \n",
    "y.shape (70000,)\n",
    "\n",
    "Explanition\n",
    "There are 70,000 images,and\teach image has 784 features.This is\tbecause\teach image is 28×28\tpixels, and\teach feature simply\trepresents one pixel’s intensity,from 0(white) to 255 (black)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-ec70ca924430>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-ec70ca924430>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    some_digit = X[36000] some_digit_image = some_digit.reshape(28,28)\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#To take a peek at one digit from the dataset\n",
    "%matplotlib\tinline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "some_digit = X[36000] some_digit_image = some_digit.reshape(28,28)\n",
    "plt.imshow(some_digit_image,cmap = matplotlib.cm.binary,\n",
    "           interpolation=\"nearest\")\n",
    "plt.axis(\"off\") \n",
    "plt.show()\n",
    "\n",
    "#Output looks like 5\n",
    "y[36000] \n",
    "#OUTPUT \n",
    "#5.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✔Split  Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create a test set and set it aside before inspecting the data closely and train set\n",
    "#The MNIST dataset is actually already split into a training set(the first 60,000images)\n",
    "#and a test set(the last 10,000 images):\n",
    "X_train, X_test, y_train, y_test = X[:60000],X[60000:],y[:60000],y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✔Shuffle the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will guarantee that all cross-validation folds will be similar\n",
    "#you don’t want\tone\tfold to be missing some digits\n",
    "#some learning algorithms are sensitive\tto the order of\tthe training instances,\tand\tthey perform poorly\tif they get many similar instances in a row. \n",
    "#Shuffling\tthe\tdataset\tensures\tthat this won’t\thappen\n",
    "\n",
    "import\tnumpy as np\n",
    "shuffle_index = np.random.permutation(60000) X_train,y_train = X_train[shuffle_index],y_train[shuffle_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Binary Classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#“5-detector”will be an example of a binary classifier, capable of distinguishing between just two classes,5 and not-5.\n",
    "#Let’s create the target vectors for this classification task\n",
    "y_train_5 = (y_train==5)#True for all 5s,False for all other digits.\n",
    "y_test_5 = (y_test==5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD) classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking a classfier and Training it\n",
    "✔A good\tplace\tto\tstart\tis\twith a Stochastic Gradient Descent (SGD) classifier,\tusing\tScikit-Learn’s\tSGDClassifier class.\tThis\tclassifier\thas\tthe\tadvantage\tof\tbeing capable\tof\thandling\tvery\tlarge\tdatasetsefficiently.\t✔This\tis\tin\tpart\tbecause\tSGD\tdeals\twith\ttraining instances\tindependently,\tone\tat\ta\ttime(which\talso\tmakes\n",
    "SGD\twell\tsuited\tfor\tonline\tlearning),\tas\twe will\tsee\tlater.\tLet’s\tcreate\tan\tSGDClassifier\tand\ttrain\tit\ton\tthe whole\ttraining\tset:\n",
    "✔The\tSGDClassifier\trelies\ton\trandomness\tduring\ttraining\t(hence\tthe\tname\t“stochastic”).\tIf\tyou\twant\treproducible\tresults,\tyou should\tset\tthe\trandom_state\tparameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train,y_train_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#you can use it to detect images of the number 5:\n",
    "sgd_clf.predict([some_digit]) \n",
    "\n",
    "#Output \n",
    "#array([\tTrue],\tdtype=bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\tthis\tModel’s\tPerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "skfolds = StratifiedKFold(n_splits=3,random_state=42)\n",
    "for train_index,test_index in skfolds.split(X_train, y_train_5):\n",
    "    clone_clf = clone(sgd_clf)X_train_folds = X_train[train_index]\n",
    "    y_train_folds = (y_train_5[train_index])\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = (y_train_5[test_index])\n",
    "        clone_clf.fit(X_train_folds, y_train_folds)\n",
    "        y_pred = clone_clf.predict(X_test_fold)\n",
    "        n_correct = sum(y_pred == y_test_fold)\n",
    "        print(n_correct\t/len(y_pred)#\tprints\t0.9502,\t0.96565\tand\t0.96495\n",
    "\n",
    "             \n",
    "              \n",
    "#The StratifiedKFold class performs stratified sampling to produce folds that contain a representative ratio of each class.\n",
    "             # At each iteration  the code creates a clone of the classifier, trains that clone on the training folds,\n",
    "              #and makes predictions on the test fold.\n",
    "              #Then it counts the number of correct predictions and outputs the ratio of correct predictions.\n",
    "              \n",
    "              \n",
    "              \n",
    "##Alternative way To do so(THE BEST WAY TO DO US)\n",
    "#Measuring Accuracy Using Cross-Validation\n",
    "from sklearn.model_selection import  cross_val_score \n",
    "cross_val_score(sgd_clf,X_train,y_train_5,cv=3,scoring=\"accuracy\") \n",
    "#array([0.9502,0.96565,0.96495])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let’s look at a very dumb classifier that just classifies every single image in the “not-5” class:\n",
    "from sklearn.base import BaseEstimator\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X,y=None):\n",
    "        pass\n",
    "    def\tpredict(self,X):\n",
    "        return np.zeros((len(X),1),dtype=bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Accuracy\n",
    "never_5_clf = Never5Classifier() \n",
    "cross_val_score(never_5_clf,X_train,y_train_5,cv=3,\tscoring=\"accuracy\")\n",
    "#Output\n",
    "#array([0.909,0.90715,0.9128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "#To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual target\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred =  cross_val_predict(sgd_clf, X_train,y_train_5,cv=3)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train_5, y_train_pred\n",
    "                   \n",
    "                 #output\n",
    "                 #array([[53272,1307],\n",
    "                       # [1077,4344]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score \n",
    "precision_score(y_train_5,y_train_pred)#==4344/(4344+1307) \n",
    "#0.76871350203503808 \n",
    "recall_score(y_train_5,y_train_pred)#== 4344/(4344+1077) \n",
    "#0.80132816823464303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It it often convenient to combine precision and recall intto a single metric called the F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train_5,\ty_train_pred) \n",
    "#0.78468208092485547\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
